{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open Cities AI teliko.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uRUnC3EWAST",
        "colab_type": "text"
      },
      "source": [
        "# **PRE-PROCESS**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmAIOTi6Z6xr",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "In the pre-process we execute : \n",
        "1.   Get tier1 and tier2 data sets from Open Cities AI challenge\n",
        "2.   Generate the labels from Geojson files\n",
        "3.   Split large Tiff images into  frames of 512 x 512 \n",
        "\n",
        "<img src=\"https://s3.amazonaws.com/drivendata-public-assets/opendri_kam_4e7c7f.png\" width=500 height=400>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BZNqQbJjy6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# libraries installation\n",
        "\n",
        "!pip install geopandas\n",
        "!apt-get install python-numpy\n",
        "!pip install solaris\n",
        "!pip install rio-tiler\n",
        "!pip install rasterio\n",
        "!pip install descartes\n",
        "!pip install pystac\n",
        "!pip install pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHjramM9X2IS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# define directories\n",
        "ROOT = '/content'\n",
        "DATA_PATH = PATH(ROOT + '/train')\n",
        "IMAGE_PATH = PATH(DATA_PATH + '/image')\n",
        "LABEL_PATH = PATH(DATA_PATH + '/label')\n",
        "\n",
        "# create directories for data\n",
        "DATA_PATH.mkdir(exist_ok= True)\n",
        "IMAGE_PATH.mkdir(exist_ok= True)\n",
        "LABEL_PATH.mkdir(exist_ok= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJnkalKJX6Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pystac import STAC_IO\n",
        "from urllib.parse import urlparse\n",
        "from pystac import (Catalog, CatalogType, Item, Asset, LabelItem, Collection)\n",
        "from pathlib import Path  \n",
        "import requests\n",
        "\n",
        "# overwriting read method of STAC_IO lib so it can handle https links\n",
        "def my_read(uri):\n",
        "    if urlparse(uri).scheme.startswith('http'):\n",
        "        return requests.get(uri).text\n",
        "    else:\n",
        "        return STAC_IO.default_read_text_method(uri)\n",
        "\n",
        "STAC_IO.read_text_method = my_read\n",
        "\n",
        "# Getting data from competition (in catalogs)\n",
        "# tier1 data set\n",
        "train_1 = Catalog.from_file('https://drivendata-competition-building-segmentation.s3-us-west-1.amazonaws.com/train_tier_1/catalog.json')\n",
        "# tier2 data set\n",
        "train_2 = Catalog.from_file('https://drivendata-competition-building-segmentation.s3-us-west-1.amazonaws.com/train_tier_2/catalog.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s26Za1ojYGnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a dict of collections for tier1 data set and tier2 data set\n",
        "train_1_col = {train_1_col.id:train_1_col for train_1_col in train_1.get_children()}\n",
        "train_2_col = {train_2_col.id:train_2_col for train_2_col in train_2.get_children()}\n",
        "\n",
        "# Generate areas for tier1\n",
        "areas_1 = []\n",
        "for col in train_1_col:\n",
        "    items = [x for x in train_1_col[col].get_all_items()]\n",
        "    for i, id in enumerate(items):\n",
        "        if i % 2 == 0 and i + 1 < len(items):\n",
        "            areas.append((col, items[i].id, items[i+1].id))\n",
        "\n",
        "# Generate areas for tier2\n",
        "areas_2 = []\n",
        "for col in train_2_col:\n",
        "    items = [x for x in train_2_col[col].get_all_items()]\n",
        "    for i, id in enumerate(items):\n",
        "        if i % 2 == 0 and i + 1 < len(items):\n",
        "            areas.append((col, items[i].id, items[i+1].id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bKbXHcEYJRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import solaris as sol\n",
        "import rasterio\n",
        "from rasterio.transform import from_bounds\n",
        "from shapely.geometry import Polygon\n",
        "from rio_tiler import main as rt_mai\n",
        "import skimage\n",
        "import os\n",
        "from rasterio.windows import Window\n",
        "import pandas as pd\n",
        "\n",
        "for area, image, label in areas_1: # CHANGE TO areas_2 TO GET TIER_2 DATA SET. \n",
        "                                   # NOT ENOUGH DISK SPACE TO PROCESS THE 2 DATA SETS AT THE SAME TIME\n",
        "  \n",
        "  items = cols[area].get_item(id=image)\n",
        "\n",
        "  # Load shapefile of each label\n",
        "  label_tmp = cols[area].get_item(id=label)\n",
        "  geo_data_frame = geopandas.read_file(label_tmp.make_asset_hrefs_absolute().assets['labels'].href)\n",
        "\n",
        "  # Get polygons from geodataframe\n",
        "  polygons = geo_data_frame.geometry\n",
        "\n",
        "  # Get outlines and zoom on each frame\n",
        "  polygon_geo = Polygon(items.to_dict()['geometry']['coordinates'][0])\n",
        "  polygon = geopandas.GeoDataFrame(index=[0], crs=geo_data_frame.crs, geometry=[polygon_geo])   \n",
        "  polygon['geometry'].to_file(image+'.geojson', driver='GeoJSON')\n",
        "  !cat {image}.geojson | supermercado burn {zoom_level} | mercantile shapes | fio collect > {img_id}{zoom_level}frames.geojson\n",
        "  \n",
        "  # Load frames and add convenience column\n",
        "  frames = geopandas.read_file(f'{image}{zoom_level}frames.geojson')\n",
        "  frames['xyz'] = frames.id.apply(lambda x: x.lstrip('(,)').rstrip('(,)').split(','))\n",
        "  frames['xyz'] = [[int(q) for q in p] for p in frames['xyz']]\n",
        "\n",
        "  # Frame url\n",
        "  frame_url = items.assets['image'].href\n",
        "  \n",
        "# SAVE FRAMES\n",
        "  x,y,z = frames['xyz']\n",
        "  frame, mask = rt_main.frame(frame_url, x,y,z, tilesize=512) # our model input is 512 x 512\n",
        "    \n",
        "  skimage.io.imsave(f'{IMAGE_PATH}/{prefix}{z}_{x}_{y}.png',np.moveaxis(frame,0,2), check_contrast=False) # save in path\n",
        "\n",
        "# SAVE MASKS\n",
        "  frame_polygon = frame['geometry']\n",
        "  tfm = from_bounds(*frame_polygon.bounds, tile_size, tile_size) \n",
        "  \n",
        "  #cropped polygons\n",
        "  polygons_crop = [poly for poly in labels_poly if poly.intersects(frame_polygon)]\n",
        "  polygons_crop_gdf = geopandas.GeoDataFrame(geometry=polygons_crop, crs=4326)\n",
        "  \n",
        "  # RGB mask\n",
        "  fbc_mask = sol.vector.mask.df_to_px_mask(df=polygons_crop_gdf,\n",
        "                                         channels=['footprint', 'boundary', 'contact'],\n",
        "                                         affine_obj=tfm, shape=(tile_size,tile_size),\n",
        "                                         boundary_width=5, boundary_type='inner', contact_spacing=5, meters=True)\n",
        "  \n",
        "  \n",
        "  skimage.io.imsave(f'{LABEL_PATH}/{prefix}{z}_{x}_{y}_mask.png',fbc_mask, check_contrast=False) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvWlOZIP-ds_",
        "colab_type": "text"
      },
      "source": [
        "# **TRAIN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyVnDi2pWGOl",
        "colab_type": "text"
      },
      "source": [
        "**The Train images are stored as large Cloud Optimized GeoTiffs (COG)**\n",
        "\n",
        "* All images include 4 bands: red, green, blue and alpha.\n",
        "\n",
        "* Spatial resolution varies from region to region\n",
        "\n",
        "Image with Label: \n",
        "\n",
        "![alt text](https://lh3.googleusercontent.com/KI2eZWRkmqgdWBs1W4ZRcy7djEAuViSuMwWJRxuE5NiIW42cQJpYyM8JSwHjw9Tf16PS=s400)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vCjwOHJ-opf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B. Basic data folder structure\n",
        "import os\n",
        "\n",
        "DATA_PATH = '/content/train'\n",
        "FRAME_PATH = DATA_PATH + '/image'\n",
        "MASK_PATH = DATA_PATH + '/label'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY6zRUbI-z9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## ONE RUN PER RESET ONLY #############################\n",
        "# C. Sort and shuffle frames and masks\n",
        "\n",
        "# Importing required libraries\n",
        "import os, random, re\n",
        "from PIL import Image\n",
        "\n",
        "# Gathering all frames and masks names\n",
        "all_frames = os.listdir(FRAME_PATH)\n",
        "all_masks = os.listdir(MASK_PATH)\n",
        "\n",
        "# Sort frames and masks\n",
        "all_frames.sort(key=lambda var:[(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
        "all_masks.sort(key=lambda var:[(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n",
        "\n",
        "# Shuffle frames and masks (with the same seed)\n",
        "random.seed(150)\n",
        "random.shuffle(all_frames)\n",
        "random.seed(150)\n",
        "random.shuffle(all_masks)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3IeOufYwgu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# D. Renaming real file names to incremental numerical file names, eg: from \n",
        "#    'e96av3.png' to '0.png' (0-based format names)\n",
        "#    name_dict associates numerical file names to real file names\n",
        "counter = 0\n",
        "name_dict = {}\n",
        "for test_file in os.listdir(DATA_PATH + '/test/image/'):\n",
        "    name_dict[str(test_file)] = str(counter)\n",
        "    os.rename(DATA_PATH + '/test/image/' + str(test_file), DATA_PATH + '/test/image/' + str(counter) + '.png')\n",
        "    counter = counter + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmqEL5No_mqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# D.2 Exporting name dict to csv (optional)\n",
        "import csv\n",
        "print(\"Saving test file name table to a file.\")\n",
        "w = csv.writer(open(\"test_file_names.csv\", \"w\"))\n",
        "for key, val in name_dict.items():\n",
        "    w.writerow([key, val])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8DZ1fTz_pWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# D.3 Reading name dict from csv (optional)\n",
        "import csv\n",
        "with open('test_file_names.csv', mode='r') as infile:\n",
        "    reader = csv.reader(infile)\n",
        "    name_dict = {rows[0]:rows[1] for rows in reader}    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqH0TPSzxdBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# E. Training dataset\n",
        "import numpy as np \n",
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import numpy as np\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#model\n",
        "def unet(weights_file_path):\n",
        "    inputs = Input((512, 512, 3))\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    \n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    drop4 = Dropout(0.5)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    up6 = BatchNormalization()(up6)\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    up7 = BatchNormalization()(up7)\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    up8 = BatchNormalization()(up8)\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    up9 = BatchNormalization()(up9)\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)    \n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(input = inputs, output = conv10)\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    if(os.path.exists(weights_file_path)): \n",
        "        model.load_weights(pretrained_weights)\n",
        "        print(\"Weights file found. Updating existing network weights.\")\n",
        "    else:\n",
        "        print(\"No weights file found. Starting training from scratch.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def train(batch_size, path):\n",
        "    #Data augmentation for our training set\n",
        "    augmentation = dict(rotation_range=0.5, width_shift_range=0.1, height_shift_range=0.1,\n",
        "                    shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
        "\n",
        "    image_datagen = ImageDataGenerator(**augmentation)\n",
        "    mask_datagen = ImageDataGenerator(**augmentation)\n",
        "\n",
        "    image_generator = image_datagen.flow_from_directory(\n",
        "        path, classes = ['image'], class_mode = None, color_mode = \"rgb\",\n",
        "        target_size = (512, 512), batch_size = batch_size, save_to_dir = None,\n",
        "        save_prefix  = \"image\", seed = 1)\n",
        "\n",
        "    mask_generator = mask_datagen.flow_from_directory(\n",
        "        path, classes = ['label'], class_mode = None, color_mode = \"grayscale\",\n",
        "        target_size = (512, 512), batch_size = batch_size, save_to_dir = None,\n",
        "        save_prefix  = \"mask\", seed = 1)\n",
        "    \n",
        "    for (img,mask) in zip(image_generator, mask_generator):\n",
        "        if(np.max(img) > 1):\n",
        "            img /= 255\n",
        "            mask /= 255\n",
        "            mask = mask > 0.5\n",
        "        yield (img,mask)\n",
        "\n",
        "# defining paths\n",
        "DATA_PATH = '/content/train'\n",
        "WEIGHTS_PATH  = '/content/checkpoint.hdf5'\n",
        "\n",
        "# defining train params\n",
        "EPOCHS = 5, BATCH_SIZE = 16\n",
        "STEPS_PER_EPOCH = (len(os.listdir(DATA_PATH + '/image'))//BATCH_SIZE)\n",
        "\n",
        "# actual training\n",
        "tr = train(BATCH_SIZE, DATA_PATH + '/')\n",
        "m = model(WEIGHTS_PATH)\n",
        "model_checkpoint = ModelCheckpoint(WEIGHTS_PATH, monitor='loss', verbose=1, save_best_only=True)\n",
        "m.fit_generator(tr, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, callbacks=[model_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI6w-T3K9zjp",
        "colab_type": "text"
      },
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc4qnCF_Ix_C",
        "colab_type": "text"
      },
      "source": [
        "**The test set contains 11.481 1024x1024 COG fragments derived from scenes that are not present in the training set.**\n",
        "\n",
        "**The submission file must contain a building footprint mask for each chip in the test set. Each mask must have the same name as its corresponding imagery chip in the test set**\n",
        "\n",
        "Mask Preview : \n",
        "\n",
        "![alt text](https://i1.wp.com/opendri.org/wp-content/uploads/2020/02/segment.png?w=400&ssl=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deEauikBnP_k",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Import \"checkpoint.hdf5\" manually**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEBTfP0Ckyqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## ONE RUN PER RESET ONLY ################################\n",
        "# A. Downloading Test dataset with 1024x1024 TIF images\n",
        "import shutil, os\n",
        "\n",
        "!wget https://drivendata-public-assets.s3.amazonaws.com/test.tgz # downloading\n",
        "!tar -xf test.tgz                                                # extracting\n",
        "if os.path.exists('/content/sample_data'):                       # removing sample_data\n",
        "    shutil.rmtree('/content/sample_data')  \n",
        "\n",
        "os.remove('/content/test.tgz')                                   #removing test.tgz                        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT1e47wgTo2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B. Changing directory names\n",
        "import os\n",
        "\n",
        "if not os.path.exists('/content/test/image'):\n",
        "    os.rename('/content/test', '/content/image')\n",
        "    os.mkdir('/content/test')\n",
        "    shutil.move('/content/image', '/content/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfMunODZk80L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## ONE RUN PER RESET ONLY ################################\n",
        "# C. Moving images from subdirectories to root directory \n",
        "import os\n",
        "\n",
        "DATA_PATH = '/content/test/image' #root directory\n",
        "\n",
        "\n",
        "for folder in os.listdir(DATA_PATH):\n",
        "    if os.path.isdir(DATA_PATH + '/' + folder):\n",
        "        shutil.move(DATA_PATH + '/' + folder + '/' + folder + '.tif', DATA_PATH)\n",
        "        shutil.rmtree(DATA_PATH + '/' + folder)\n",
        "\n",
        "print('Number of Images : ' + len(os.listdir(DATA_PATH)))\n",
        "print(os.listdir(DATA_PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwb7yMi3YcRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## ONE RUN PER RESET ONLY ################################\n",
        "# D. Renaming real file names to incremental numerical file names, eg: from \n",
        "#    'e96av3.png' to '0.png' (0-based format names)\n",
        "\n",
        "file_names = {}\n",
        "counter = 0\n",
        "for file in os.listdir(DATA_PATH):\n",
        "        file_names[counter] = file\n",
        "        os.rename(DATA_PATH + '/' + str(file) , DATA_PATH + '/' + str(counter) + '.png')\n",
        "        counter += 1\n",
        "        \n",
        "print(os.listdir(DATA_PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG0LSW0_Z6MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# E. Removing 4th channel (alpha channel) from TIF images (This may take a while...)\n",
        "import cv2, os\n",
        "for test_file in os.listdir(DATA_PATH):\n",
        "    img = cv2.imread(DATA_PATH + '/' + test_file, 1) # reading images\n",
        "    if not img is None:  # if image is readable, saves it as a 3-channel image\n",
        "        cv2.imwrite(DATA_PATH + '/' + test_file, img)\n",
        "    else: # if image file is corrupted (broken), remove file altogether\n",
        "        os.remove(DATA_PATH + '/' + test_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCZP83IB17xP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# F. Runs prediction model and saves the results\n",
        "import skimage.io as io\n",
        "import numpy as np\n",
        "import skimage.transform as transform\n",
        "from skimage import img_as_ubyte\n",
        "import os\n",
        "\n",
        "# defines a test generator\n",
        "def test(path):\n",
        "    for i in range(os.listdir(path)):\n",
        "        img = io.imread(os.path.join(path,\"%d.png\"%i), as_gray = True)\n",
        "        img = img / 255\n",
        "        img = transform.resize(img,(512, 512))\n",
        "        img = np.reshape(img,(1,)+img.shape)\n",
        "        yield img\n",
        "\n",
        "# define paths\n",
        "WEIGHTS_PATH = '/content/checkpoint.hdf5'\n",
        "OUTPUT_PATH = '/content/test/results'\n",
        "if not os.path.exists(OUTPUT_PATH): os.mkdir(OUTPUT_PATH)\n",
        "\n",
        "# pick mode, create a test generator and make predictions\n",
        "m = model(WEIGHTS_PATH)\n",
        "te = test(DATA_PATH)\n",
        "results = model.predict_generator(te, len(os.listdir(DATA_PATH)), verbose = 1)\n",
        "\n",
        "# saving results\n",
        "for i,item in enumerate(results):\n",
        "    img_old = item[:,:,0]\n",
        "    img = img_old > 0.5\n",
        "    io.imsave(os.path.join(path,\"%d_predict.png\"%i),img_as_ubyte(img))\n",
        "    img = io.imread(os.path.join(path,\"%d_predict.png\"%i),img_as_ubyte(img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C-uhBwpq4js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################## ONE RUN PER RESET ONLY ################################\n",
        "# G. Post procesing - Resizing final images from 512x512 back to 1024x1024\n",
        "import cv2, os, ast\n",
        "\n",
        "# define submissions path\n",
        "SUBMISSIONS_PATH = '/content/submissions'\n",
        "if not os.path.exists(SUBMISSIONS_PATH): os.mkdir(SUBMISSIONS_PATH)\n",
        "\n",
        "# resizing + renaming\n",
        "for file in os.listdir(OUTPUT_PATH):\n",
        "    if file.endswith('.png'):\n",
        "        img = cv2.imread(OUTPUT_PATH + '/' + file, cv2.IMREAD_UNCHANGED)\n",
        "        resized = cv2.resize(img, (1024, 1024), interpolation = cv2.INTER_AREA)\n",
        "        cv2.imwrite(SUBMISSIONS_PATH + '/' + file_names[int(file[:file.index('_')])] , resized)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}